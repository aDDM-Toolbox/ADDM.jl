{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyDDM and ChaRTr\n",
    "\n",
    "- My current understanding is that the way we write our likelihood functions is solving the FPE using Forward Euler. So\n",
    "1. Make sure that our defaults for state and time step size are not in what Shinn et al. identify as unstable regions \n",
    "2. Can we re-write them to implement a Backward Euler or Crank Nicholson solution and then extract the probability from the time bin we are interested in (ie. the one that corresponds to the observed RT)? \n",
    "  - Possibly but they are difficult to implement\n",
    "\n",
    "- Forward Euler, Backward Euler and Crank Nicholson are *not* parameter estimation methods. They are to compute the PDF of the RT conditional on choice. This PDF is then used for likelihood based estimation. This is what we do too but (I think) we don't use the \"full distribution\" to compute our trial likelihoods. Instead, we're only interested in the probability in a specific time bin.\n",
    "  - The nice thing about Forward Euler is that it is easy to implement, visualize and examine\n",
    "  - By using Julia we can keep these nice things without being slowed down by the computation time.\n",
    "- What they refer to as \"full distribution likelihood methods\" are for parameter estimation, following the computation of the RT PDF. For us, this is grid search (what they in another context call parameter sweep).\n",
    "\n",
    "- Both Shinn et al. and Chandrasekaran & Hawkins use the Roitman and Shadlen data as test cases. Both use these data to argue the need for the flexibility to capture \"other\" cognitive mechanisms that are not part of the traditional DDM framework.\n",
    "  - Should we use it for computational speed comparison?\n",
    "\n",
    "- Neither pyDDM nor ChaRTr have posteriors for parameters. ChaRTr's model comparison is limited to either assuming all model are equally likely or dubios AIC\n",
    "\n",
    "- ChaRTr also uses a (strange) QMP statistic instead of likelihood for parameter estimation\n",
    "\n",
    "- What is the timestep size used in ChaRTr for trialwise simulations\n",
    "\n",
    "- One suggestion for Wenning based on these papers: Instead of using Nelder-Mead simplex, how about trying differential evolution, which is what both papers prefer\n",
    "\n",
    "- What is the default parameter space size for ChaRTr and pyDDM. They both use differential evolution as preferred algorithm. How often does any particle actually run up to max iterations?\n",
    "  - ChaRTr: 200 particles, max 750 iterations. Does this mean max 200*750 = 150000 parameter combinations \n",
    "  - pyDDM: number of particles: 15 * number of parameters, max iterations 1000\n",
    "\n",
    "- Processing speed has two components:\n",
    "  1. Computation of likelihood (or whatever statistic is used for estimation) - this is what is reported in Shinn et al. in Figure 4.\n",
    "  2. Algortihm testing out different parameter combinations. In both ChaRTr and pyDDM this is \"outsourced\" to another package to implement the differential evolution algorithm\n",
    "\n",
    "- One thing we don't address as a package using likelihood for estimation: Sensitivity to outliers, which is solved by contaminants (ie. assuming some portion of the data is generated by noise alone), as explained by Shinn et al. on p 11.\n",
    "\n",
    "- (Similar to pyDDM) we have a numerical solver (Fokker Planck) and numerical (?) fitter"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
