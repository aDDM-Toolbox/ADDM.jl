{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ~~Check if all jobs ran for the 8000 param combination version~~ Yes.  \n",
    "- ~~Re submit all jobs with the same resources~~  \n",
    "  - ~~Add nthreads column to outputs~~  \n",
    "  - ~~Don't overwrite previous results~~  \n",
    "  - ~~Re run this notebook with benchmark results from jobs ran with identical resources~~  \n",
    "- Decide on `DistributedEx()` within `Transducers` vs `Dagger.jl` vs `MPI.jl`\n",
    "  - Still reluctant on `MPI.jl` because it seems like it would require a lot of work to test various use cases would work well and require very careful considerations of how to move data around nodes etc.\n",
    "  - If you do decide to stick with the `Transducers` ecosystem then [this section on containerization seems important](https://juliafolds.github.io/Transducers.jl/dev/tutorials/tutorial_parallel/#tutorial-parallel-collect). Important point is to use something like `init = Union{}[]` to initialize where the outputs of the `collect` will be stored.\n",
    "  ~~- Having read a bit more about `Dagger.jl` I'm now leaning towards it, if I see any performance improvements over the hybrid of `Base.threads` + `Floops.ThreadedEx()`~~\n",
    "  - Having read even more and seeing only performance decrease with `Dagger.jl` for `compute_trials_nll` I'm leaning away from it. **Also, and more importantly, can't parallelize the grid search if you want to get trial posteriors** So there is no reason to implement grid_search_dagger versions as they can't offer the same functionality as other parallelization options.\n",
    "  - So is there any reason to run more benchmarks using Dagger for compute_trials_nll\n",
    "    - `grid_search_benchmarks_outputs` and `small_stepsize_recovery.ipynb` suggest opposing results for what is the best combination of `Floops` and `Base.threads`. So running everything one more time, with same resources, smaller stepsize, functions that are similar to each other in terms of output types and compared to serial might be useful to have as a comparison\n",
    "  - Conclusions:\n",
    "- Change functions in toolbox to the final parallelized versions  \n",
    "- ~~Try recovery on the same dataset used for grid_search benchmarks using a smaller step size (.01)~~ \n",
    "  ~~- [submitted again 4/1 - 12:30pm]: Preliminary resultss based on stdout timestamps suggests that using ThreadedEx() for grid_search + Base.Threads for trials is 5 hours faster.~~\n",
    "- What is the most user-friendly but explicit and error-proof workflow to implement this toolbox?\n",
    "  - Would using a config be good? What can be specified in config:\n",
    "    - grid_search arguments: grid_exec, trials_exec, fixed_params, likelihood_args, return_grid_nlls, return_model_posteriors, return_trial_posteriors, model_priors\n",
    "    - data file\n",
    "    - param_grid file\n",
    "- Benchmark multiple subjects in \n",
    "  - 1. julia script loop using distributed executor \n",
    "  - 2. job array  \n",
    "~~- How to extract more details about the job distributions across threads?~~\n",
    "  ~~- This might be interesting to me as I'm trying to optimize performance but unlikely to be useful for users of the toolbox for scientific discovery~~\n",
    "- Fix tutorials\n",
    "  - Check/fix downstream functions (e.g. marginal_posteriors) to see if they work with the new outputs of grid_search\n",
    "- Add parallelization tutorial\n",
    "  - Currently, ADDM.jl relies on some embarassingly parallel data parallelization based on ... By default, ... When using on HPCs\n",
    "  - Sample batch scripts\n",
    "  - Brute force multi node parallelization + manual post-processing?\n",
    "  - Include a note on `MPI.jl` to the parallelization tutorial. \n",
    "  - Include this discussion: https://discourse.julialang.org/t/is-clustermanagers-jl-maintained-or-how-to-do-multi-node-calculations-in-julia/110050/24\n",
    "  - Include this tutorial: https://enccs.github.io/julia-for-hpc/motivation-hpc/\n",
    "- From the literature figure out: (esp pyDDM, HSSM and ChaRTr)\n",
    "  - Is there a model we can use to compare all packages to each other?\n",
    "  - What parallelization options do existing packages have?\n",
    "  - What are the test cases (size of data, size of parameter space, computing resources) reported\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
