<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Model comparison · ADDM.jl</title><meta name="title" content="Model comparison · ADDM.jl"/><meta property="og:title" content="Model comparison · ADDM.jl"/><meta property="twitter:title" content="Model comparison · ADDM.jl"/><meta name="description" content="Documentation for ADDM.jl."/><meta property="og:description" content="Documentation for ADDM.jl."/><meta property="twitter:description" content="Documentation for ADDM.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="ADDM.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">ADDM.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../01_getting_started/">Getting started with ADDM.jl</a></li><li><a class="tocitem" href="../03_empirical_data/">Parameter estimation on empirical data</a></li><li class="is-active"><a class="tocitem" href>Model comparison</a><ul class="internal"><li><a class="tocitem" href="#Comparing-parameters-of-a-single-generative-processes"><span>Comparing parameters of a single generative processes</span></a></li><li><a class="tocitem" href="#Comparing-different-generative-processes"><span>Comparing different generative processes</span></a></li><li><a class="tocitem" href="#Comparing-true-data-with-simulated-data"><span>Comparing true data with simulated data</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../../apireference/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Model comparison</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Model comparison</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/aDDM-Toolbox/ADDM.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/aDDM-Toolbox/ADDM.jl/blob/main/docs/src/tutorials/04_model_comparison.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Model-comparison"><a class="docs-heading-anchor" href="#Model-comparison">Model comparison</a><a id="Model-comparison-1"></a><a class="docs-heading-anchor-permalink" href="#Model-comparison" title="Permalink"></a></h1><p>The parameter combination that has the highest likelihood to have generated a given dataset (the maximum likelihood estimate) is often what is used in downstream analyses and related to other variables of interest. While a fast estimation these parameters is therefore very useful, it is valueable to get a sense of the uncertainty associated with the estimation as well. In this tutorial we introduce some of the toolbox&#39;s capabilities to assess this.</p><p>When estimating the best-fitting parameters for a model (aDDM or otherwise) our ability to recover them is <em>always</em> limited to the parameter space we explore. Therefore, any computation of the uncertainty associated with specific parameters values is only with respect to other values that we have tried.</p><p>In other words, the uncertainty is not some divine measure that accounts for all possible models. It is a comparative measure that tells us how much better a specific combination of parameters is, compared to other combinations in the parameter space we have defined. In this toolbox, we make the parameter space explicit by specifying the grid (<code>param_grid</code>) in the <code>ADDM.grid_search</code> function. </p><p>The uncertainty associated with each parameter value and/or parameter combination is quantified as a probability distribution. Specifically, a posterior probability distribution that reflects both the prior beliefs on how likely each parameter value is and how much to update them based on the evidence each trial provides in favor of a parameter combination.</p><h2 id="Comparing-parameters-of-a-single-generative-processes"><a class="docs-heading-anchor" href="#Comparing-parameters-of-a-single-generative-processes">Comparing parameters of a single generative processes</a><a id="Comparing-parameters-of-a-single-generative-processes-1"></a><a class="docs-heading-anchor-permalink" href="#Comparing-parameters-of-a-single-generative-processes" title="Permalink"></a></h2><p>In this section we will demonstrate how to compute posterior probabilities associated with each parameter combination and each parameter type for a single generative process. A generative process, in this context, refers to the computational model we believe gives rise to observable data (in this case, choices and response times). Here, we compute the uncertainty over different parameter combinations of one specific computational model, the standard aDDM. In the next section we compute the uncertainty over different computational models, accounting for the uncertainty within the parameter spaces of each model.</p><h3 id="Posterior-model-probability"><a class="docs-heading-anchor" href="#Posterior-model-probability">Posterior model probability</a><a id="Posterior-model-probability-1"></a><a class="docs-heading-anchor-permalink" href="#Posterior-model-probability" title="Permalink"></a></h3><p>We begin with importing the packages that will be used in this tutorial.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using ADDM, CSV, DataFrames, DataFramesMeta, Distributed, Distributions, LinearAlgebra, StatsPlots</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>The toolbox comes with a subset of the data from Krajbich et al. (2010). In this tutorials we will use data from a single subject from this dataset.</p><pre><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; krajbich_data = ADDM.load_data_from_csv(data_path * &quot;Krajbich2010_behavior.csv&quot;, data_path * &quot;Krajbich2010_fixations.csv&quot;);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; subj_data = krajbich_data[&quot;18&quot;];</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>To examine the uncertainty associated with each parameter and their combinations we introduce the <code>return_model_posteriors</code> argument when running <code>ADDM.grid_search</code>, which expands the output to include a <code>trial_posteriors</code> dictionary. <code>trial_posteriors</code> is indexed by the keys of <code>param_grid</code> as indicators of different parameter combinations and contains the posterior probability for each key after each trial as its values.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; fn = data_path * &quot;Krajbich_grid3.csv&quot;;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; tmp = DataFrame(CSV.File(fn, delim=&quot;,&quot;));</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; param_grid = NamedTuple.(eachrow(tmp));</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; my_likelihood_args = (timeStep = 10.0, stateStep = 0.01);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; output = ADDM.grid_search(subj_data, param_grid, ADDM.aDDM_get_trial_likelihood,
           Dict(:η=&gt;0.0, :barrier=&gt;1, :decay=&gt;0, :nonDecisionTime=&gt;0, :bias=&gt;0.0),
           likelihood_args=my_likelihood_args,
           return_grid_nlls = true, return_trial_posteriors = true, return_model_posteriors = true);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; mle = output[:mle];</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; nll_df = output[:grid_nlls];</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; trial_posteriors = output[:trial_posteriors];</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; model_posteriors = output[:model_posteriors];</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p><code>model_posteriors</code> contains the posterior probability associated with each model (i.e. parameter combination) <strong>for the set of models that were fit</strong>. Since it is a probability distribution it must sum to 1. In other words, the posterior probabilities associated with the models would change if they were being compared to different combinations of parameters, because they would be renormalized with respect to a different set of likelihoods.</p></div></div><h3 id="Model-posteriors"><a class="docs-heading-anchor" href="#Model-posteriors">Model posteriors</a><a id="Model-posteriors-1"></a><a class="docs-heading-anchor-permalink" href="#Model-posteriors" title="Permalink"></a></h3><p>The <code>model_posteriors</code> variable is a dictionary indexed by the parameter combination as listed in the <code>param_grid</code>. Here, we convert that <code>model_posteriors</code> dictionary to a dataframe so it is easier to make plots with.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; posteriors_df1 = DataFrame();</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; for (k, v) in model_posteriors
         cur_row = DataFrame([k])
         cur_row.posterior = [v]
         posteriors_df1 = vcat(posteriors_df1, cur_row, cols=:union)
       end;</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>Now we can visualize the posterior probability for each parameter combination. </p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p><code>DataFrameMeta.jl</code> provides functionality similar to the R package <code>dplyr</code> (e.g. <code>@chain</code> is similar to a piping operation and <code>@rsubset</code> to <code>select</code>.  </p></div></div><p>Below we&#39;re only plotting the posteriors for models that have a meaningful amount of probability mass instead of all the models that were tested by excluding rows without a posterior probability greater than <code>1e-10</code>.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; plot_df = @chain posteriors_df1 begin
         @rsubset :posterior &gt; 1e-10
         @rtransform :x_label = &quot;d: &quot; * string(:d) * &quot;, \nσ: &quot; * string(:sigma) * &quot;, \nθ: &quot; * string(:theta)
         @orderby -:posterior
         end;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; sort(posteriors_df1, :posterior, rev=true)</code><code class="nohighlight hljs ansi" style="display:block;"><span class="sgr1">64×4 DataFrame
 Row │ d        sigma    theta    posterior
     │<span class="sgr90"> Float64  Float64  Float64  Float64
─────┼────────────────────────────────────────
   1 │ 0.00085    0.055      0.5  0.392581
   2 │ 0.00085    0.055      0.3  0.321245
   3 │ 0.00075    0.055      0.3  0.0950647
   4 │ 0.00075    0.055      0.5  0.0851215
   5 │ 0.00085    0.055      0.7  0.0547531
   6 │ 0.00065    0.055      0.3  0.0143631
   7 │ 0.00075    0.055      0.7  0.0141713
   8 │ 0.00065    0.055      0.5  0.0102092
  ⋮  │    ⋮        ⋮        ⋮          ⋮
  58 │ 0.00085    0.035      0.9  1.08703e-22
  59 │ 0.00075    0.035      0.9  2.7816e-23
  60 │ 0.00055    0.035      0.3  6.43983e-24
  61 │ 0.00055    0.035      0.5  5.07393e-24
  62 │ 0.00065    0.035      0.9  1.53731e-24
  63 │ 0.00055    0.035      0.7  7.14635e-25
  64 │ 0.00055    0.035      0.9  1.83497e-26
</span><span class="sgr36">                               49 rows omitted</span></span></code></pre><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; @df plot_df bar(:x_label, :posterior, legend = false, xrotation = 45, ylabel = &quot;p(model|data)&quot;,bottom_margin = (5, :mm))</code><code class="nohighlight hljs ansi" style="display:block;">Plot{Plots.GRBackend() n=1}</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p><img src="../plot_3_1.png" alt="plot"/></p><h4 id="Trialwise-changes-to-the-model-posteriors"><a class="docs-heading-anchor" href="#Trialwise-changes-to-the-model-posteriors">Trialwise changes to the model posteriors</a><a id="Trialwise-changes-to-the-model-posteriors-1"></a><a class="docs-heading-anchor-permalink" href="#Trialwise-changes-to-the-model-posteriors" title="Permalink"></a></h4><p>The <code>ADDM.grid_search</code> function&#39;s <code>return_trial_posteriors</code> argument returns the discretized posterior distribution for each model after each <em>trial/observation</em>. This allows us to examine how the posterior distribution changes accounting for increasing amounts of data. The <code>trial_posteriors</code> key of the <code>grid_search</code> output is organized as a dictionary with keys indicating parameter combinations from <code>param_grid</code> and values are nested dictionaries mapping trial numbers to posterior probabilities.</p><p>To do so, first we rangle the <code>trial_posteriors</code> into a data frame for easier visualization.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; # Initialize empty df
       trial_posteriors_df = DataFrame();</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; nTrials = length(subj_data)</code><code class="nohighlight hljs ansi" style="display:block;">100</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; for i in 1:nTrials
       
         # Get the posterior for each model after the curent trial
         cur_trial_posteriors = DataFrame(keys(trial_posteriors))
         cur_trial_posteriors[!, :posterior] = [x[i] for x in values(trial_posteriors)]
       
         # Add the trial information
         cur_trial_posteriors[!, :trial_num] .= i
       
         # Add the current trial posterior to the initialized df
         trial_posteriors_df = vcat(trial_posteriors_df, cur_trial_posteriors, cols=:union)
       end;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; @transform!(trial_posteriors_df, @byrow :modelnum = string(:d) * string(:sigma) * string(:theta))</code><code class="nohighlight hljs ansi" style="display:block;"><span class="sgr1">6400×6 DataFrame
  Row │ d        sigma    theta    posterior    trial_num  modelnum
      │<span class="sgr90"> Float64  Float64  Float64  Float64      Int64      String
──────┼────────────────────────────────────────────────────────────────────
    1 │ 0.00055    0.035      0.9  0.0366842            1  0.000550.0350.9
    2 │ 0.00065    0.035      0.7  0.0382042            1  0.000650.0350.7
    3 │ 0.00075    0.045      0.3  0.0188203            1  0.000750.0450.3
    4 │ 0.00065    0.055      0.9  0.00551948           1  0.000650.0550.9
    5 │ 0.00065    0.035      0.3  0.0411853            1  0.000650.0350.3
    6 │ 0.00065    0.035      0.5  0.0404291            1  0.000650.0350.5
    7 │ 0.00065    0.065      0.9  0.00139241           1  0.000650.0650.9
    8 │ 0.00075    0.045      0.9  0.0153804            1  0.000750.0450.9
  ⋮   │    ⋮        ⋮        ⋮          ⋮           ⋮             ⋮
 6394 │ 0.00065    0.065      0.5  6.53903e-7         100  0.000650.0650.5
 6395 │ 0.00055    0.035      0.5  5.07393e-24        100  0.000550.0350.5
 6396 │ 0.00075    0.045      0.7  3.98117e-5         100  0.000750.0450.7
 6397 │ 0.00085    0.035      0.9  1.08703e-22        100  0.000850.0350.9
 6398 │ 0.00055    0.065      0.9  4.03303e-9         100  0.000550.0650.9
 6399 │ 0.00075    0.045      0.5  0.000381812        100  0.000750.0450.5
 6400 │ 0.00065    0.065      0.3  8.54979e-7         100  0.000650.0650.3
</span><span class="sgr36">                                                          6385 rows omitted</span></span></code></pre><p>Then, plot changes to posteriors of each model across trials. Note, we have omitted a legend indicating the parameters associated with each line in the plot below to avoid over-crowding the plot. This is meant only as an intial exploration into how the conclusions about the best model vary with increased evidence from each trial.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; @df trial_posteriors_df plot(
             :trial_num,
             :posterior,
             group = :modelnum,
             xlabel = &quot;Trial&quot;,
             ylabel = &quot;Posterior p&quot;,
             legend = false
         )</code><code class="nohighlight hljs ansi" style="display:block;">Plot{Plots.GRBackend() n=64}</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p><img src="../plot_3_2.png" alt="plot"/></p><h3 id="Parameter-posteriors"><a class="docs-heading-anchor" href="#Parameter-posteriors">Parameter posteriors</a><a id="Parameter-posteriors-1"></a><a class="docs-heading-anchor-permalink" href="#Parameter-posteriors" title="Permalink"></a></h3><p>The <code>model_posteriors</code> dictionary contains the probability distribution associated with each parameter combination. The <code>ADDM.marginal_posteriors</code> function summarizes this by collapsing over levels of different parameters. Below, we first summarize the distribution for each of the three parameters separately.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; param_posteriors = ADDM.marginal_posteriors(model_posteriors);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; plot_array = Any[];</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; for plot_df in param_posteriors
         x_lab = names(plot_df)[1]
         cur_plot = @df plot_df bar(plot_df[:, x_lab], :posterior_sum, leg = false, ylabel = &quot;p(&quot; * x_lab * &quot; = x|data)&quot;, xlabel = x_lab )
         push!(plot_array, cur_plot)
       end;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; plot(plot_array...)</code><code class="nohighlight hljs ansi" style="display:block;">Plot{Plots.GRBackend() n=3}</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p><img src="../plot_3_3.png" alt="plot"/></p><p>We can also use the <code>ADDM.marginal_posteriors</code> function to compute parameter posteriors with respect to each other by specifying the second positional argument. When set to <code>true</code>, the <code>ADDM.marginal_posteriors</code> function returns pairwise marginal distributions that can be plotted as heatmaps to visualize conditional distributions of the parameters.   </p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; all_marginal_posteriors = ADDM.marginal_posteriors(model_posteriors, two_d_marginals = true)</code><code class="nohighlight hljs ansi" style="display:block;">6-element Vector{Any}:
 <span class="sgr1">4×2 DataFrame
 Row │ d        posterior_sum
     │<span class="sgr90"> Float64  Float64
─────┼────────────────────────
   1 │ 0.00055     0.00197559
   2 │ 0.00065     0.0268011
   3 │ 0.00075     0.195637
   4 │ 0.00085     0.775586
 4×2 DataFrame
 Row │ sigma    posterior_sum
     │ Float64  Float64
─────┼────────────────────────
   1 │   0.035    9.57592e-18
   2 │   0.045    0.00696685
   3 │   0.055    0.993005
   4 │   0.065    2.82679e-5
 4×2 DataFrame
 Row │ theta    posterior_sum
     │ Float64  Float64
─────┼────────────────────────
   1 │     0.9     0.00147894
   2 │     0.7     0.0714554
   3 │     0.3     0.434736
   4 │     0.5     0.49233
 16×3 DataFrame
 Row │ d        sigma    posterior_sum
     │ Float64  Float64  Float64
─────┼─────────────────────────────────
   1 │ 0.00055    0.035    1.22467e-23
   2 │ 0.00065    0.035    4.40377e-21
   3 │ 0.00075    0.045    0.000826574
   4 │ 0.00065    0.055    0.0267524
   5 │ 0.00065    0.065    1.70294e-6
   6 │ 0.00085    0.045    0.00609219
   7 │ 0.00075    0.055    0.194803
   8 │ 0.00075    0.065    7.17705e-6
   9 │ 0.00055    0.045    1.10675e-6
  10 │ 0.00075    0.035    3.97713e-19
  11 │ 0.00085    0.055    0.769475
  12 │ 0.00065    0.045    4.69828e-5
  13 │ 0.00085    0.065    1.91349e-5
  14 │ 0.00055    0.065    2.53055e-7
  15 │ 0.00055    0.055    0.00197423
  16 │ 0.00085    0.035    9.17379e-18
 16×3 DataFrame
 Row │ d        theta    posterior_sum
     │ Float64  Float64  Float64
─────┼─────────────────────────────────
   1 │ 0.00055      0.9    1.71243e-5
   2 │ 0.00065      0.7    0.00206368
   3 │ 0.00075      0.3    0.0954724
   4 │ 0.00065      0.9    0.000119287
   5 │ 0.00065      0.3    0.0143895
   6 │ 0.00065      0.5    0.0102286
   7 │ 0.00075      0.9    0.000446247
   8 │ 0.00085      0.7    0.0550113
   9 │ 0.00075      0.7    0.0142119
  10 │ 0.00085      0.3    0.323762
  11 │ 0.00075      0.5    0.0855065
  12 │ 0.00085      0.5    0.395917
  13 │ 0.00055      0.7    0.000168505
  14 │ 0.00085      0.9    0.000896282
  15 │ 0.00055      0.5    0.000678258
  16 │ 0.00055      0.3    0.00111171
 16×3 DataFrame
 Row │ sigma    theta    posterior_sum
     │ Float64  Float64  Float64
─────┼─────────────────────────────────
   1 │   0.035      0.9    1.38075e-22
   2 │   0.035      0.7    2.13653e-19
   3 │   0.045      0.3    0.00293996
   4 │   0.055      0.9    0.00147704
   5 │   0.035      0.3    2.98907e-18
   6 │   0.035      0.5    6.37305e-18
   7 │   0.065      0.9    1.47174e-7
   8 │   0.045      0.9    1.74939e-6
   9 │   0.045      0.7    0.000298579
  10 │   0.055      0.7    0.0711537
  11 │   0.055      0.3    0.431784
  12 │   0.065      0.5    1.35042e-5
  13 │   0.045      0.5    0.00372656
  14 │   0.065      0.7    3.0881e-6
  15 │   0.055      0.5    0.48859
  16 │   0.065      0.3    1.15284e-5</span></span></code></pre><p>The toolbox includes a visualization function, <code>ADDM.marginal_posterior_plot</code> that creates a grid of plots with individual parameter posteriors on the diagonal and the conditional posteriors as heatmaps below the diagonal.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; ADDM.marginal_posterior_plot(all_marginal_posteriors)</code><code class="nohighlight hljs ansi" style="display:block;">Plot{Plots.GRBackend() n=6}</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p><img src="../plot_3_4.png" alt="plot"/></p><h4 id="Trialwise-changes-to-the-parameter-posteriors"><a class="docs-heading-anchor" href="#Trialwise-changes-to-the-parameter-posteriors">Trialwise changes to the parameter posteriors</a><a id="Trialwise-changes-to-the-parameter-posteriors-1"></a><a class="docs-heading-anchor-permalink" href="#Trialwise-changes-to-the-parameter-posteriors" title="Permalink"></a></h4><p>Similar to trialwise changes for combinations of parameters, we can also examine trialwise changes to marginalized posteriors for each individual parameter as well. Here we do so by using <code>ADDM.marginal_posteriors</code> for each entry in <code>trial_posteriors</code>.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; # Initialize empty df
       trial_param_posteriors = DataFrame();</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; for i in 1:nTrials
       
         # Get the posterior for each model after the curent trial
         cur_trial_posteriors = Dict(zip(keys(trial_posteriors), [x[i] for x in values(trial_posteriors)]))
       
         # Use built-in function to marginalize for each parameter
         cur_param_posteriors = ADDM.marginal_posteriors(cur_trial_posteriors)
       
         # Wrangle the output to be a single df and add trial number info
         for j in 1:length(cur_param_posteriors)
           df = cur_param_posteriors[j][:,:] #assign a copy
       
           df[!, :par_name] .= names(df)[1]
           df[!, :trial_num] .= i
           rename!(df, Symbol(names(df)[1]) =&gt; :par_value)
       
           trial_param_posteriors = vcat(trial_param_posteriors, df, cols=:union)
       
         end
       
       end</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>Plot trialwise marginal posteriors for each parameter</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; par_names = unique(trial_param_posteriors[:,:par_name]);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; plot_array = Any[];</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; for cur_par_name in par_names
       
         plot_df = @rsubset(trial_param_posteriors, :par_name == cur_par_name)
       
         cur_plot = @df plot_df plot(
             :trial_num,
             :posterior_sum,
             group = :par_value,
             title = cur_par_name,
             xlabel = &quot;Trial&quot;,
             ylabel = &quot;Posterior p&quot;,
         )
       
         push!(plot_array, cur_plot)
       
       end</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; plot(plot_array...)</code><code class="nohighlight hljs ansi" style="display:block;">Plot{Plots.GRBackend() n=12}</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p><img src="../plot_3_5.png" alt="plot"/></p><h2 id="Comparing-different-generative-processes"><a class="docs-heading-anchor" href="#Comparing-different-generative-processes">Comparing different generative processes</a><a id="Comparing-different-generative-processes-1"></a><a class="docs-heading-anchor-permalink" href="#Comparing-different-generative-processes" title="Permalink"></a></h2><p>Aside from comparing different parameter combinations for a single model, we can also compare how likely one computational model is compared to another, in generating the observed data. Since any specific value of a given parameter involves uncertainty as we computed above, we need to account for this when comparing different generative processes to each other.</p><p>This again involves computing the comparative advantage, the posterior probability, for each point in the parameter space that we examine, which contains both the parameters within each model, <em>and</em> which model they belong to. </p><p>Here, we&#39;ll use the same participant&#39;s data from before and examine if it can be explained better by a standard aDDM (that we fit above) or another model where the boundaries of the evidence accummulation decay exponentially throughout the decision. This model is detailed further in the <a href="https://addm-toolbox.github.io/ADDM.jl/dev/tutorials/custom_model/">Defining custom models</a> tutorial.</p><p>The comparison of these two generative processes is operationalized by specifying them in the same <code>param_grid</code> as we had previously used to specify different values for the parameters of a single generative process. In this case, we add the information on which generative process the parameter combination belongs to in a new key called <code>likelihood_fn</code>.</p><p>First we read in the file that defines the parameter space for the first model, the standard aDDM.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; fn1 = data_path * &quot;Krajbich_grid3.csv&quot;;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; tmp = DataFrame(CSV.File(fn1, delim=&quot;,&quot;));</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; tmp.likelihood_fn .= &quot;ADDM.aDDM_get_trial_likelihood&quot;;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; param_grid1 = NamedTuple.(eachrow(tmp));</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>Then we define the likelihood function for the second model. We do this by reading in a custom function we have defined in a separate script. This script includes a function called <code>my_likelihood_fn</code>. We will use this function name string when defining the parameter space.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p><code>@everywhere</code> is a macro defined by <code>Distributed.jl</code>. It ensures that the likelihood function is available to all processes when parallelizing computations.</p></div></div><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; @everywhere include(data_path * &quot;my_likelihood_fn.jl&quot;);</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: TaskFailedException

<span class="sgr91">    nested task error: </span>UndefVarError: `data_path` not defined
    Stacktrace:
     [1] top-level scope
    <span class="sgr90">   @</span> <span class="sgr90"><span class="sgr4">none:1</span></span>
     [2] <span class="sgr1">eval</span>
    <span class="sgr90">   @</span> <span class="sgr90">./<span class="sgr4">boot.jl:370</span> [inlined]</span>
     [3] <span class="sgr1">(::Distributed.var&quot;#172#174&quot;{Module, Expr})()</span>
    <span class="sgr90">   @</span> <span class="sgr35">Distributed</span> <span class="sgr90">./<span class="sgr4">task.jl:514</span></span></code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>Now we define the parameter space we will examine for the second model. In addition to the parameter values we also include <code>my_likelihood_fn</code> as a string in <code>param_grid</code> so <code>ADDM.grid_search</code> knows which generative process to use when computing the trial likelihoods for the parameter combinations of the second model. </p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; fn2 = data_path * &quot;custom_model_grid.csv&quot;;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; tmp = DataFrame(CSV.File(fn2, delim=&quot;,&quot;));</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; tmp.likelihood_fn .= &quot;my_likelihood_fn&quot;;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; param_grid2 = NamedTuple.(eachrow(tmp));</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>Now that we have defined the parameter space for both models, we combine them both in a single <code>param_grid</code>, over which we&#39;ll compute the posterior distribution.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; param_grid = vcat(param_grid1, param_grid2)</code><code class="nohighlight hljs ansi" style="display:block;">145-element Vector{NamedTuple}:
 (d = 0.00055, sigma = 0.035, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;)
 (d = 0.00065, sigma = 0.035, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;)
 (d = 0.00075, sigma = 0.035, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;)
 (d = 0.00085, sigma = 0.035, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;)
 (d = 0.00055, sigma = 0.045, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;)
 (d = 0.00065, sigma = 0.045, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;)
 (d = 0.00075, sigma = 0.045, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;)
 (d = 0.00085, sigma = 0.045, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;)
 (d = 0.00055, sigma = 0.055, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;)
 (d = 0.00065, sigma = 0.055, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;)
 ⋮
 (d = 0.003, sigma = 0.01, theta = 0.9, lambda = 0.1, likelihood_fn = &quot;my_likelihood_fn&quot;)
 (d = 0.007, sigma = 0.01, theta = 0.9, lambda = 0.1, likelihood_fn = &quot;my_likelihood_fn&quot;)
 (d = 0.014, sigma = 0.01, theta = 0.9, lambda = 0.1, likelihood_fn = &quot;my_likelihood_fn&quot;)
 (d = 0.003, sigma = 0.03, theta = 0.9, lambda = 0.1, likelihood_fn = &quot;my_likelihood_fn&quot;)
 (d = 0.007, sigma = 0.03, theta = 0.9, lambda = 0.1, likelihood_fn = &quot;my_likelihood_fn&quot;)
 (d = 0.014, sigma = 0.03, theta = 0.9, lambda = 0.1, likelihood_fn = &quot;my_likelihood_fn&quot;)
 (d = 0.003, sigma = 0.07, theta = 0.9, lambda = 0.1, likelihood_fn = &quot;my_likelihood_fn&quot;)
 (d = 0.007, sigma = 0.07, theta = 0.9, lambda = 0.1, likelihood_fn = &quot;my_likelihood_fn&quot;)
 (d = 0.014, sigma = 0.07, theta = 0.9, lambda = 0.1, likelihood_fn = &quot;my_likelihood_fn&quot;)</code></pre><p>With this expanded <code>param_grid</code> that includes information on the different likelihood functions we call the <code>ADDM.grid_search</code> function setting the third position argument to <code>nothing</code>. This argument is where we define the likelihood function in the case of a single model but now this is specified in the <code>param_grid</code>.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; my_likelihood_args = (timeStep = 10.0, stateStep = 0.01);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; output = ADDM.grid_search(subj_data, param_grid, nothing,
           Dict(:η=&gt;0.0, :barrier=&gt;1, :decay=&gt;0, :nonDecisionTime=&gt;0, :bias=&gt;0.0),
           likelihood_args = my_likelihood_args,
           return_grid_nlls = true, return_trial_posteriors = true, return_model_posteriors = true);</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `my_likelihood_fn` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; mle = output[:mle]</code><code class="nohighlight hljs ansi" style="display:block;">Dict{Symbol, Real} with 13 entries:
  :barrier         =&gt; 1
  :decay           =&gt; 0
  :timeStep        =&gt; 10.0
  :nonDecisionTime =&gt; 0
  :σ               =&gt; 0.055
  :sigma           =&gt; 0.055
  :η               =&gt; 0.0
  :bias            =&gt; 0.0
  :d               =&gt; 0.00085
  :nll             =&gt; 689.553
  :θ               =&gt; 0.5
  :stateStep       =&gt; 0.01
  :theta           =&gt; 0.5</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; nll_df = output[:grid_nlls]</code><code class="nohighlight hljs ansi" style="display:block;"><span class="sgr1">64×4 DataFrame
 Row │ d        sigma    theta    nll
     │<span class="sgr90"> Float64  Float64  Float64  Float64
─────┼────────────────────────────────────
   1 │ 0.00055    0.035      0.9  747.878
   2 │ 0.00065    0.035      0.7  738.646
   3 │ 0.00075    0.045      0.3  696.431
   4 │ 0.00065    0.055      0.9  697.652
   5 │ 0.00065    0.035      0.3  736.227
   6 │ 0.00065    0.035      0.5  736.226
   7 │ 0.00065    0.065      0.9  706.505
   8 │ 0.00075    0.045      0.9  703.184
  ⋮  │    ⋮        ⋮        ⋮        ⋮
  58 │ 0.00065    0.065      0.5  702.858
  59 │ 0.00055    0.035      0.5  742.256
  60 │ 0.00075    0.045      0.7  698.749
  61 │ 0.00085    0.035      0.9  739.191
  62 │ 0.00055    0.065      0.9  707.946
  63 │ 0.00075    0.045      0.5  696.488
  64 │ 0.00065    0.065      0.3  702.59
</span><span class="sgr36">                           49 rows omitted</span></span></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; trial_posteriors = output[:trial_posteriors];</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; model_posteriors = output[:model_posteriors];</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>As before, we create a dataframe containing the <code>model_posteriors</code> for visualization purposes.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; posteriors_df2 = DataFrame();</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; for (k, v) in model_posteriors
         cur_row = DataFrame([k])
         cur_row.posterior = [v]
         posteriors_df2 = vcat(posteriors_df2, cur_row, cols=:union)
       end;</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>We can take a look at the most likely parameter combinations across the generative processes.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; sort(posteriors_df2, :posterior, rev=true)</code><code class="nohighlight hljs ansi" style="display:block;"><span class="sgr1">64×4 DataFrame
 Row │ d        sigma    theta    posterior
     │<span class="sgr90"> Float64  Float64  Float64  Float64
─────┼────────────────────────────────────────
   1 │ 0.00085    0.055      0.5  0.392581
   2 │ 0.00085    0.055      0.3  0.321245
   3 │ 0.00075    0.055      0.3  0.0950647
   4 │ 0.00075    0.055      0.5  0.0851215
   5 │ 0.00085    0.055      0.7  0.0547531
   6 │ 0.00065    0.055      0.3  0.0143631
   7 │ 0.00075    0.055      0.7  0.0141713
   8 │ 0.00065    0.055      0.5  0.0102092
  ⋮  │    ⋮        ⋮        ⋮          ⋮
  58 │ 0.00085    0.035      0.9  1.08703e-22
  59 │ 0.00075    0.035      0.9  2.7816e-23
  60 │ 0.00055    0.035      0.3  6.43983e-24
  61 │ 0.00055    0.035      0.5  5.07393e-24
  62 │ 0.00065    0.035      0.9  1.53731e-24
  63 │ 0.00055    0.035      0.7  7.14635e-25
  64 │ 0.00055    0.035      0.9  1.83497e-26
</span><span class="sgr36">                               49 rows omitted</span></span></code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The posterior probability associated with the standard model for parameters <code>d = 0.00085</code>, <code>sigma = 0.055</code>  and <code>theta =  0.5</code>  is <strong>not</strong> the same as what it was when comparing the parameter combinations for a single generative process in the first section of this tutorial. Now, this posterior is normalized not only over the parameter combinations of the standard model but also over all the combinations that we examined for the alternative model.</p></div></div><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; sort(posteriors_df1, :posterior, rev=true)[1,:posterior] == sort(posteriors_df2, :posterior, rev=true)[1,:posterior]</code><code class="nohighlight hljs ansi" style="display:block;">true</code></pre><p>We can also collapse the posterior distribution across the generative processes and compare how much better one processes is compared to the other in giving rise to the observed data.  </p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; gdf = groupby(posteriors_df2, :likelihood_fn);</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: ArgumentError: column name :likelihood_fn not found in the data frame</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; combdf = combine(gdf, :posterior =&gt; sum);</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `gdf` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; @df combdf bar(:likelihood_fn, :posterior_sum, legend = false, xrotation = 45, ylabel = &quot;p(model|data)&quot;,bottom_margin = (5, :mm))</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `combdf` not defined</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p><img src="../plot_3_6.png" alt="plot"/></p><p>We can check how this conclusion evolved with the addition of each trial.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; # Initialize empty df
       trial_model_posteriors = DataFrame();</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; for i in 1:nTrials
       
         # Get the posterior for each model after the curent trial
         cur_trial_posteriors = Dict(zip(keys(trial_posteriors), [x[i] for x in values(trial_posteriors)]))
       
         cur_trial_posteriors = DataFrame(model_num = collect(keys(cur_trial_posteriors)), posterior = collect(values(cur_trial_posteriors)))
       
         @transform!(cur_trial_posteriors, @byrow :likelihood_fn = :model_num.likelihood_fn)
       
         gdf = groupby(cur_trial_posteriors, :likelihood_fn)
         cur_trial_posteriors = combine(gdf, :posterior =&gt; sum)
       
         # Add the trial information
         cur_trial_posteriors[!, :trial_num] .= i
       
         # Add the current trial posterior to the initialized df
         trial_model_posteriors = vcat(trial_model_posteriors, cur_trial_posteriors, cols=:union)
       end;</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: type NamedTuple has no field likelihood_fn</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; @df trial_model_posteriors plot(
             :trial_num,
             :posterior_sum,
             group = :likelihood_fn,
             xlabel = &quot;Trial&quot;,
             ylabel = &quot;Posterior p&quot;,
             legend = true
         )</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: MethodError: no method matching _extract_group_attributes(::Symbol, ::Symbol, ::Symbol)

Closest candidates are:
  _extract_group_attributes(<span class="sgr91">::AbstractVector</span>, ::Any...; legend_entry)
<span class="sgr90">   @</span> <span class="sgr36">RecipesPipeline</span> <span class="sgr90">~/.julia/packages/RecipesPipeline/BGM3l/src/<span class="sgr4">group.jl:10</span></span>
  _extract_group_attributes(<span class="sgr91">::Tuple</span>, ::Any...)
<span class="sgr90">   @</span> <span class="sgr36">RecipesPipeline</span> <span class="sgr90">~/.julia/packages/RecipesPipeline/BGM3l/src/<span class="sgr4">group.jl:27</span></span>
  _extract_group_attributes(<span class="sgr91">::NamedTuple</span>, ::Any...)
<span class="sgr90">   @</span> <span class="sgr36">RecipesPipeline</span> <span class="sgr90">~/.julia/packages/RecipesPipeline/BGM3l/src/<span class="sgr4">group.jl:36</span></span>
  ...</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p><img src="../plot_3_7.png" alt="plot"/></p><h3 id="Priors-about-models"><a class="docs-heading-anchor" href="#Priors-about-models">Priors about models</a><a id="Priors-about-models-1"></a><a class="docs-heading-anchor-permalink" href="#Priors-about-models" title="Permalink"></a></h3><p>Suppose we had very strong prior beliefs about two of the models in our parameter space. We specify this belief as a probability of <code>.495</code> for two models and assign the remaining probability mass to all other models.</p><p>Important: </p><ol><li>Make sure the keys of the model priors dictionary has the same keys for all models (<code>ADDM.match_param_grid_keys</code>).</li><li>Make sure there is some probability mass for all models (i.e. all values in model priors dictionary should be larger than 0.).</li><li>Make sure that the values of model priors sum up to 1 (i.e. so it a proper probability distribution).</li></ol><p>This is not a good example to demonstrate the effect of priors because the evidence against the custom model is too strong immediately after the first trial.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; param_grid = ADDM.match_param_grid_keys(param_grid)</code><code class="nohighlight hljs ansi" style="display:block;">145-element Vector{Any}:
 (d = 0.00055, sigma = 0.035, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;, lambda = &quot;NA&quot;)
 (d = 0.00065, sigma = 0.035, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;, lambda = &quot;NA&quot;)
 (d = 0.00075, sigma = 0.035, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;, lambda = &quot;NA&quot;)
 (d = 0.00085, sigma = 0.035, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;, lambda = &quot;NA&quot;)
 (d = 0.00055, sigma = 0.045, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;, lambda = &quot;NA&quot;)
 (d = 0.00065, sigma = 0.045, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;, lambda = &quot;NA&quot;)
 (d = 0.00075, sigma = 0.045, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;, lambda = &quot;NA&quot;)
 (d = 0.00085, sigma = 0.045, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;, lambda = &quot;NA&quot;)
 (d = 0.00055, sigma = 0.055, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;, lambda = &quot;NA&quot;)
 (d = 0.00065, sigma = 0.055, theta = 0.3, likelihood_fn = &quot;ADDM.aDDM_get_trial_likelihood&quot;, lambda = &quot;NA&quot;)
 ⋮
 (d = 0.003, sigma = 0.01, theta = 0.9, likelihood_fn = &quot;my_likelihood_fn&quot;, lambda = 0.1)
 (d = 0.007, sigma = 0.01, theta = 0.9, likelihood_fn = &quot;my_likelihood_fn&quot;, lambda = 0.1)
 (d = 0.014, sigma = 0.01, theta = 0.9, likelihood_fn = &quot;my_likelihood_fn&quot;, lambda = 0.1)
 (d = 0.003, sigma = 0.03, theta = 0.9, likelihood_fn = &quot;my_likelihood_fn&quot;, lambda = 0.1)
 (d = 0.007, sigma = 0.03, theta = 0.9, likelihood_fn = &quot;my_likelihood_fn&quot;, lambda = 0.1)
 (d = 0.014, sigma = 0.03, theta = 0.9, likelihood_fn = &quot;my_likelihood_fn&quot;, lambda = 0.1)
 (d = 0.003, sigma = 0.07, theta = 0.9, likelihood_fn = &quot;my_likelihood_fn&quot;, lambda = 0.1)
 (d = 0.007, sigma = 0.07, theta = 0.9, likelihood_fn = &quot;my_likelihood_fn&quot;, lambda = 0.1)
 (d = 0.014, sigma = 0.07, theta = 0.9, likelihood_fn = &quot;my_likelihood_fn&quot;, lambda = 0.1)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; n_models = length(param_grid)</code><code class="nohighlight hljs ansi" style="display:block;">145</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; my_priors = Dict(zip(param_grid, repeat([(1-(.495*2))/(n_models)], outer = n_models)))</code><code class="nohighlight hljs ansi" style="display:block;">Dict{Any, Float64} with 145 entries:
  (d = 0.014, sigma = 0.01, theta = 0.6, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  (d = 0.007, sigma = 0.03, theta = 0.6, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  (d = 0.014, sigma = 0.01, theta = 0.6, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  (d = 0.00085, sigma = 0.045, theta = 0.3, likelihood_fn = &quot;ADDM… =&gt; 6.89655e-5
  (d = 0.00075, sigma = 0.065, theta = 0.5, likelihood_fn = &quot;ADDM… =&gt; 6.89655e-5
  (d = 0.003, sigma = 0.03, theta = 0.0, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  (d = 0.007, sigma = 0.07, theta = 0.9, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  (d = 0.014, sigma = 0.03, theta = 0.9, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  (d = 0.00055, sigma = 0.045, theta = 0.3, likelihood_fn = &quot;ADDM… =&gt; 6.89655e-5
  (d = 0.014, sigma = 0.07, theta = 0.0, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  (d = 0.00085, sigma = 0.045, theta = 0.9, likelihood_fn = &quot;ADDM… =&gt; 6.89655e-5
  (d = 0.007, sigma = 0.01, theta = 0.9, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  (d = 0.007, sigma = 0.03, theta = 0.0, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  (d = 0.00075, sigma = 0.035, theta = 0.3, likelihood_fn = &quot;ADDM… =&gt; 6.89655e-5
  (d = 0.014, sigma = 0.01, theta = 0.0, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  (d = 0.003, sigma = 0.03, theta = 0.6, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  (d = 0.014, sigma = 0.07, theta = 0.9, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  (d = 0.003, sigma = 0.01, theta = 0.0, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  (d = 0.003, sigma = 0.07, theta = 0.9, likelihood_fn = &quot;my_like… =&gt; 6.89655e-5
  ⋮                                                                =&gt; ⋮</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; my_priors[(d = 0.014, sigma = 0.07, theta = 0.9, lambda = 0.01, likelihood_fn = &quot;my_likelihood_fn&quot;)] = .495</code><code class="nohighlight hljs ansi" style="display:block;">0.495</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; my_priors[(d = 0.014, sigma = 0.07, theta = 0.6, lambda = 0.01, likelihood_fn = &quot;my_likelihood_fn&quot;)] = .495</code><code class="nohighlight hljs ansi" style="display:block;">0.495</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; output = ADDM.grid_search(subj_data, param_grid, nothing,
           Dict(:η=&gt;0.0, :barrier=&gt;1, :decay=&gt;0, :nonDecisionTime=&gt;0, :bias=&gt;0.0),
           likelihood_args = my_likelihood_args,
           model_priors = my_priors,
           return_grid_nlls = true, return_trial_posteriors = true, return_model_posteriors = true);</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `my_likelihood_fn` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; mle = output[:mle]</code><code class="nohighlight hljs ansi" style="display:block;">Dict{Symbol, Real} with 13 entries:
  :barrier         =&gt; 1
  :decay           =&gt; 0
  :timeStep        =&gt; 10.0
  :nonDecisionTime =&gt; 0
  :σ               =&gt; 0.055
  :sigma           =&gt; 0.055
  :η               =&gt; 0.0
  :bias            =&gt; 0.0
  :d               =&gt; 0.00085
  :nll             =&gt; 689.553
  :θ               =&gt; 0.5
  :stateStep       =&gt; 0.01
  :theta           =&gt; 0.5</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; nll_df = output[:grid_nlls]</code><code class="nohighlight hljs ansi" style="display:block;"><span class="sgr1">64×4 DataFrame
 Row │ d        sigma    theta    nll
     │<span class="sgr90"> Float64  Float64  Float64  Float64
─────┼────────────────────────────────────
   1 │ 0.00055    0.035      0.9  747.878
   2 │ 0.00065    0.035      0.7  738.646
   3 │ 0.00075    0.045      0.3  696.431
   4 │ 0.00065    0.055      0.9  697.652
   5 │ 0.00065    0.035      0.3  736.227
   6 │ 0.00065    0.035      0.5  736.226
   7 │ 0.00065    0.065      0.9  706.505
   8 │ 0.00075    0.045      0.9  703.184
  ⋮  │    ⋮        ⋮        ⋮        ⋮
  58 │ 0.00065    0.065      0.5  702.858
  59 │ 0.00055    0.035      0.5  742.256
  60 │ 0.00075    0.045      0.7  698.749
  61 │ 0.00085    0.035      0.9  739.191
  62 │ 0.00055    0.065      0.9  707.946
  63 │ 0.00075    0.045      0.5  696.488
  64 │ 0.00065    0.065      0.3  702.59
</span><span class="sgr36">                           49 rows omitted</span></span></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; trial_posteriors = output[:trial_posteriors];</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; model_posteriors = output[:model_posteriors];</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><h2 id="Comparing-true-data-with-simulated-data"><a class="docs-heading-anchor" href="#Comparing-true-data-with-simulated-data">Comparing true data with simulated data</a><a id="Comparing-true-data-with-simulated-data-1"></a><a class="docs-heading-anchor-permalink" href="#Comparing-true-data-with-simulated-data" title="Permalink"></a></h2><p>The comparison of the generative processes above strongly favors the standard aDDM over the custom model in generating the observed data (within the ranges of the parameter space we explored).</p><p>Another way to examine how well a model describes observed data is by comparing how well it predicts observed patterns. In this case, this would involve inspecting response time distributions conditional on choice, as these are the two outputs of the generative models.</p><p>One can choose different features and statistics about the observed data to compare with model predictions. Below, we plot how the response time distributions for the best fitting model from each generative process compares to the true data.  </p><p>First, we get best fitting parameters for each model.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; bestModelPars = @chain posteriors_df2 begin
           groupby(:likelihood_fn)
           combine(_) do sdf
               sdf[argmax(sdf.posterior), :]
           end
         end;</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: ArgumentError: column name :likelihood_fn not found in the data frame</code></pre><p>Using these parameters for each model we simulate data for the stimuli used in the true data.</p><p>We begin with preparing the inputs for the simulating function. These are the fixation data, simulator arguments and the stimuli.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; vDiffs = sort(unique([x.valueLeft - x.valueRight for x in subj_data]));</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; fixData = ADDM.process_fixations(krajbich_data, fixDistType=&quot;fixation&quot;, valueDiffs = vDiffs);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; MyArgs = (timeStep = 10.0, cutOff = 20000, fixationData = fixData);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; MyStims = (valueLeft = [x.valueLeft for x in subj_data], valueRight = [x.valueRight for x in subj_data])</code><code class="nohighlight hljs ansi" style="display:block;">(valueLeft = [2, 8, 3, 4, 8, 5, 5, 4, 9, 6  …  6, 8, 5, 2, 4, 6, 7, 6, 6, 5], valueRight = [0, 8, 3, 3, 5, 0, 5, 5, 5, 2  …  5, 6, 7, 6, 3, 6, 5, 9, 5, 8])</code></pre><p>Then, we define the standard model with the best fitting parameters.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; standPars = @rsubset bestModelPars :likelihood_fn == &quot;ADDM.aDDM_get_trial_likelihood&quot;;</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `bestModelPars` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; standModel = ADDM.define_model(d = standPars.d[1], σ = standPars.sigma[1], θ = standPars.theta[1]);</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `standPars` not defined</code></pre><p>Now that the model and the inputs for the simulator are defined we can simulate data.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; simStand = ADDM.simulate_data(standModel, MyStims, ADDM.aDDM_simulate_trial, MyArgs);</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `standModel` not defined</code></pre><p>We repeat these steps for the alternative model. The simulator function for this model is defined in <code>my_trial_simulator.jl</code> so we need to source that into our session before we can call the function.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; @everywhere include(&quot;./my_trial_simulator.jl&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: TaskFailedException

<span class="sgr91">    nested task error: </span>SystemError: opening file &quot;/home/runner/work/ADDM.jl/ADDM.jl/docs/my_trial_simulator.jl&quot;: No such file or directory
    Stacktrace:
      [1] <span class="sgr1">systemerror(</span><span class="sgr90">p</span>::String, <span class="sgr90">errno</span>::Int32; <span class="sgr90">extrainfo</span>::Nothing<span class="sgr1">)</span>
    <span class="sgr90">    @</span> <span class="sgr90">Base</span> <span class="sgr90">./<span class="sgr4">error.jl:176</span></span>
      [2] <span class="sgr1">#systemerror#82</span>
    <span class="sgr90">    @</span> <span class="sgr90">./<span class="sgr4">error.jl:175</span> [inlined]</span>
      [3] <span class="sgr1">systemerror</span>
    <span class="sgr90">    @</span> <span class="sgr90">./<span class="sgr4">error.jl:175</span> [inlined]</span>
      [4] <span class="sgr1">open(</span><span class="sgr90">fname</span>::String; <span class="sgr90">lock</span>::Bool, <span class="sgr90">read</span>::Nothing, <span class="sgr90">write</span>::Nothing, <span class="sgr90">create</span>::Nothing, <span class="sgr90">truncate</span>::Nothing, <span class="sgr90">append</span>::Nothing<span class="sgr1">)</span>
    <span class="sgr90">    @</span> <span class="sgr90">Base</span> <span class="sgr90">./<span class="sgr4">iostream.jl:293</span></span>
      [5] <span class="sgr1">open</span>
    <span class="sgr90">    @</span> <span class="sgr90">./<span class="sgr4">iostream.jl:275</span> [inlined]</span>
      [6] <span class="sgr1">open(</span><span class="sgr90">f</span>::Base.var&quot;#418#419&quot;<span class="sgr90">{String}</span>, <span class="sgr90">args</span>::String; <span class="sgr90">kwargs</span>::Base.Pairs<span class="sgr90">{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}</span><span class="sgr1">)</span>
    <span class="sgr90">    @</span> <span class="sgr90">Base</span> <span class="sgr90">./<span class="sgr4">io.jl:393</span></span>
      [7] <span class="sgr1">open</span>
    <span class="sgr90">    @</span> <span class="sgr90">./<span class="sgr4">io.jl:392</span> [inlined]</span>
      [8] <span class="sgr1">read</span>
    <span class="sgr90">    @</span> <span class="sgr90">./<span class="sgr4">io.jl:473</span> [inlined]</span>
      [9] <span class="sgr1">_include(</span><span class="sgr90">mapexpr</span>::Function, <span class="sgr90">mod</span>::Module, <span class="sgr90">_path</span>::String<span class="sgr1">)</span>
    <span class="sgr90">    @</span> <span class="sgr90">Base</span> <span class="sgr90">./<span class="sgr4">loading.jl:1959</span></span>
     [10] <span class="sgr1">include(</span><span class="sgr90">fname</span>::String<span class="sgr1">)</span>
    <span class="sgr90">    @</span> <span class="sgr90">Base.MainInclude</span> <span class="sgr90">./<span class="sgr4">client.jl:478</span></span>
     [11] top-level scope
    <span class="sgr90">    @</span> <span class="sgr90"><span class="sgr4">none:1</span></span>
     [12] <span class="sgr1">eval</span>
    <span class="sgr90">    @</span> <span class="sgr90">./<span class="sgr4">boot.jl:370</span> [inlined]</span>
     [13] <span class="sgr1">(::Distributed.var&quot;#172#174&quot;{Module, Expr})()</span>
    <span class="sgr90">    @</span> <span class="sgr35">Distributed</span> <span class="sgr90">./<span class="sgr4">task.jl:514</span></span></code></pre><p>Now we can define the alternative model with the best fitting parameters for that model and simulate data.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; ## Define standard model with the best fitting parameters
       altPars = @rsubset bestModelPars :likelihood_fn == &quot;my_likelihood_fn&quot;;</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `bestModelPars` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; altModel = ADDM.define_model(d = altPars.d[1], σ = altPars.sigma[1], θ = altPars.theta[1])</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `altPars` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; altModel.λ = altPars.lambda[1];</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `altPars` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ## Simulate data for the best alternative model
       simAlt = ADDM.simulate_data(altModel, MyStims, my_trial_simulator, MyArgs);</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `altModel` not defined</code></pre><p>Now that we have simulated data using both generative processes, we can plot the response time data for the true and simulated data. We will visualize this as histograms and kernel density estimates of RT distributions conditional on choice. The RTs for left choices will be on the left side of the plot and vice versa for right choice RTs. For visualization purposes the left choice RTs are multiplied by -1.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; # Plot true RT histograms overlaid with simulated RT histograms
       
       ## Define the limit for the x-axis based on true data
       rts = [i.RT * i.choice for i in subj_data]; #left choice rt&#39;s are negative</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; l = abs(minimum(rts)) &gt; abs(maximum(rts)) ? abs(minimum(rts)) : abs(maximum(rts))</code><code class="nohighlight hljs ansi" style="display:block;">13456</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ## Split the RTs for left and right choice. Left is on the left side of the plot
       rts_pos = [i.RT for i in subj_data if i.choice &gt; 0];</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; rts_neg = [i.RT * (-1) for i in subj_data if i.choice &lt; 0];</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; rts_pos_stand = [i.RT for i in simStand if i.choice &gt; 0];</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `simStand` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; rts_pos_alt = [i.RT for i in simAlt if i.choice &gt; 0];</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `simAlt` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; rts_neg_stand = [i.RT * (-1) for i in simStand if i.choice &lt; 0];</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `simStand` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; rts_neg_alt = [i.RT * (-1) for i in simAlt if i.choice &lt; 0];</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `simAlt` not defined</code></pre><p>Having extracted the data for both the true and simulated RTs we can plot them on top each other. </p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; ## Make plot
       
       histogram(rts_pos, normalize=true, bins = range(-l, l, length=41), fillcolor = &quot;gray&quot;, yaxis = false, grid = false, label = &quot;True data&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">Plot{Plots.GRBackend() n=1}</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; density!(rts_pos_stand, label = &quot;ADDM predictions&quot;, linewidth = 3, linecolor = &quot;blue&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `rts_pos_stand` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; density!(rts_pos_alt, label = &quot;Custom model predictions&quot;, linewidth = 3, linecolor = &quot;green&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `rts_pos_alt` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; histogram!(rts_neg, normalize=true, bins = range(-l, l, length=41), fillcolor = &quot;gray&quot;, label = &quot;&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">Plot{Plots.GRBackend() n=2}</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; density!(rts_neg_stand, linewidth = 3, linecolor = &quot;blue&quot;, label = &quot;&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `rts_neg_stand` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; density!(rts_neg_alt, linewidth = 3, linecolor = &quot;green&quot;, label = &quot;&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: `rts_neg_alt` not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; vline!([0], linecolor = &quot;red&quot;, label = &quot;&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">Plot{Plots.GRBackend() n=3}</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p><img src="../plot_3_8.png" alt="plot"/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../03_empirical_data/">« Parameter estimation on empirical data</a><a class="docs-footer-nextpage" href="../../apireference/">API Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.0 on <span class="colophon-date" title="Monday 29 April 2024 22:23">Monday 29 April 2024</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
